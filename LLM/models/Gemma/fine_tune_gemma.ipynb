{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -q -U bitsandbytes==0.42.0\n",
    "# !pip3 install -q -U peft==0.8.2\n",
    "# !pip3 install -q -U trl==0.7.10\n",
    "# !pip3 install -q -U accelerate==0.27.1\n",
    "# !pip3 install -q -U datasets==2.17.0\n",
    "# !pip3 install -q -U transformers==4.38.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niq/hcsr2001/anaconda3/envs/LLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/niq/hcsr2001/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    " \n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niq/hcsr2001/anaconda3/envs/LLM/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "# model_id = \"google/gemma-7b\"\n",
    "# model_id = \"google/gemma-2b-it\"\n",
    "# model_id = \"google/gemma-2b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "    device = \"cuda:0\"\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    <start_of_turn>user\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "    {query}\n",
    "    <end_of_turn>\\n<start_of_turn>model\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(query=query)\n",
    "\n",
    "    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    # decoded = tokenizer.batch_decode(generated_ids)\n",
    "    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return (decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    user\n",
      "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "    code the fibonacci series in python using reccursion\n",
      "    \n",
      "model\n",
      "    \n",
      "\n",
      "    def fibonacci(n):\n",
      "        if n <= 1:\n",
      "            return n\n",
      "        else:\n",
      "            return fibonacci(n-1) + fibonacci(n-2)\n"
     ]
    }
   ],
   "source": [
    "result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 8.18k/8.18k [00:00<00:00, 25.5MB/s]\n",
      "Downloading data: 100%|██████████| 32.4M/32.4M [00:02<00:00, 13.2MB/s]\n",
      "Downloading data: 100%|██████████| 1.90M/1.90M [00:00<00:00, 4.96MB/s]\n",
      "Generating train split: 100000 examples [00:00, 871178.02 examples/s]\n",
      "Generating test split: 5851 examples [00:00, 709909.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>domain_description</th>\n",
       "      <th>sql_complexity</th>\n",
       "      <th>sql_complexity_description</th>\n",
       "      <th>sql_task_type</th>\n",
       "      <th>sql_task_type_description</th>\n",
       "      <th>sql_prompt</th>\n",
       "      <th>sql_context</th>\n",
       "      <th>sql</th>\n",
       "      <th>sql_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5097</td>\n",
       "      <td>forestry</td>\n",
       "      <td>Comprehensive data on sustainable forest manag...</td>\n",
       "      <td>single join</td>\n",
       "      <td>only one join (specify inner, outer, cross)</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>What is the total volume of timber sold by eac...</td>\n",
       "      <td>CREATE TABLE salesperson (salesperson_id INT, ...</td>\n",
       "      <td>SELECT salesperson_id, name, SUM(volume) as to...</td>\n",
       "      <td>Joins timber_sales and salesperson tables, gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5098</td>\n",
       "      <td>defense industry</td>\n",
       "      <td>Defense contract data, military equipment main...</td>\n",
       "      <td>aggregation</td>\n",
       "      <td>aggregation functions (COUNT, SUM, AVG, MIN, M...</td>\n",
       "      <td>analytics and reporting</td>\n",
       "      <td>generating reports, dashboards, and analytical...</td>\n",
       "      <td>List all the unique equipment types and their ...</td>\n",
       "      <td>CREATE TABLE equipment_maintenance (equipment_...</td>\n",
       "      <td>SELECT equipment_type, SUM(maintenance_frequen...</td>\n",
       "      <td>This query groups the equipment_maintenance ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id            domain                                 domain_description  \\\n",
       "0  5097          forestry  Comprehensive data on sustainable forest manag...   \n",
       "1  5098  defense industry  Defense contract data, military equipment main...   \n",
       "\n",
       "  sql_complexity                         sql_complexity_description  \\\n",
       "0    single join        only one join (specify inner, outer, cross)   \n",
       "1    aggregation  aggregation functions (COUNT, SUM, AVG, MIN, M...   \n",
       "\n",
       "             sql_task_type                          sql_task_type_description  \\\n",
       "0  analytics and reporting  generating reports, dashboards, and analytical...   \n",
       "1  analytics and reporting  generating reports, dashboards, and analytical...   \n",
       "\n",
       "                                          sql_prompt  \\\n",
       "0  What is the total volume of timber sold by eac...   \n",
       "1  List all the unique equipment types and their ...   \n",
       "\n",
       "                                         sql_context  \\\n",
       "0  CREATE TABLE salesperson (salesperson_id INT, ...   \n",
       "1  CREATE TABLE equipment_maintenance (equipment_...   \n",
       "\n",
       "                                                 sql  \\\n",
       "0  SELECT salesperson_id, name, SUM(volume) as to...   \n",
       "1  SELECT equipment_type, SUM(maintenance_frequen...   \n",
       "\n",
       "                                     sql_explanation  \n",
       "0  Joins timber_sales and salesperson tables, gro...  \n",
       "1  This query groups the equipment_maintenance ta...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gretelai/synthetic_text_to_sql\",split=\"train[:10%]\")\n",
    "dataset.to_pandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction Finetuning\n",
    "\n",
    "To prepare the dataset for finetuning, follow these steps:\n",
    "\n",
    "1. **Generate Prompt:** Use the `generate_prompt` function to create prompts from instructions and outputs.\n",
    "2. **Shuffle Dataset:** Randomly shuffle the dataset to ensure diversity in training examples.\n",
    "3. **Tokenize Dataset:** Tokenize the dataset using a tokenizer.\n",
    "\n",
    "#### Formatting the Dataset\n",
    "\n",
    "Format the dataset according to the following Gemma instruction format:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt: Given the following SQL question and context, generate a SQL query:\n",
    "\n",
    "# SQL Question: [SQL_QUESTION]\n",
    "# SQL Context: [SQL_CONTEXT]\n",
    "\n",
    "# Generated SQL Query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "    :param data_point: dict: Data point\n",
    "    :return: dict: tokenzed prompt\n",
    "    \"\"\"\n",
    "    prefix_text = 'Given the following SQL question and context, generate a SQL query:\\n\\n'\n",
    "    # Samples with additional context into.\n",
    "    if data_point['sql_context']:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} SQL Question: {data_point[\"sql_prompt\"]} \\n\\n SQL Context: {data_point[\"sql_context\"]}\\n\\n Generated SQL Query: <end_of_turn>\\n\\n<start_of_turn> model {data_point[\"sql\"]} <end_of_turn>\"\"\"\n",
    "    # Without\n",
    "    else:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} SQL Question: {data_point[\"sql_prompt\"]} \\n\\n Generated SQL Query: <end_of_turn>\\n<start_of_turn>model {data_point[\"sql\"]} <end_of_turn>\"\"\"\n",
    "    return text\n",
    "\n",
    "# add the \"prompt\" column in the dataset\n",
    "text_column = [generate_prompt(data_point) for data_point in dataset]\n",
    "dataset = dataset.add_column(\"prompt\", text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation', 'prompt'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 7234.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user Given the following SQL question and context, generate a SQL query:\n",
      "\n",
      " SQL Question: Determine the percentage of threat intelligence budget allocated to cybersecurity in the last 6 months \n",
      "\n",
      " SQL Context: CREATE TABLE threat_intelligence_budget (category TEXT, allocation_date DATE, budget_amount NUMERIC); INSERT INTO threat_intelligence_budget (category, allocation_date, budget_amount) VALUES ('Cybersecurity', '2022-01-01', 50000), ('Physical Security', '2022-01-01', 30000), ('Personnel Security', '2022-01-01', 20000), ('Cybersecurity', '2022-02-01', 55000), ('Physical Security', '2022-02-01', 35000), ('Personnel Security', '2022-02-01', 25000), ('Cybersecurity', '2022-03-01', 60000), ('Physical Security', '2022-03-01', 30000), ('Personnel Security', '2022-03-01', 20000);\n",
      "\n",
      " Generated SQL Query: <end_of_turn>\n",
      "\n",
      "<start_of_turn> model SELECT (SUM(CASE WHEN category = 'Cybersecurity' THEN budget_amount ELSE 0 END) * 100.0 / SUM(budget_amount)) as cybersecurity_percentage FROM threat_intelligence_budget WHERE allocation_date >= (SELECT CURRENT_DATE - INTERVAL '6 months'); <end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v_proj', 'up_proj', 'down_proj', 'k_proj', 'q_proj', 'o_proj', 'gate_proj']\n"
     ]
    }
   ],
   "source": [
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 78446592 | total: 2584619008 | Percentage: 3.0351%\n"
     ]
    }
   ],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    " \n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"gemma-7b-dolly-chatml\", # directory to save and repository id\n",
    "#     num_train_epochs=3,                     # number of training epochs\n",
    "#     per_device_train_batch_size=2,          # batch size per device during training\n",
    "#     gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "#     gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "#     optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "#     logging_steps=10,                       # log every 10 steps\n",
    "#     save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "#     bf16=True,                              # use bfloat16 precision\n",
    "#     tf32=True,                              # use tf32 precision\n",
    "#     learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "#     max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "#     warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "#     lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "#     push_to_hub=False,                       # push model to hub\n",
    "#     report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niq/hcsr2001/anaconda3/envs/LLM/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/niq/hcsr2001/anaconda3/envs/LLM/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 8000/8000 [00:00<00:00, 10042.97 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 11909.12 examples/s]\n",
      "/home/niq/hcsr2001/anaconda3/envs/LLM/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#new code using SFTTrainer\n",
    "import transformers\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=0.03,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niq/hcsr2001/anaconda3/envs/LLM/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/niq/hcsr2001/anaconda3/envs/LLM/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 13:07, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.424200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.098800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.933700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.934100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.723300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.736200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.720600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.732700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.775300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.742700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.731600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.698400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.574300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.643800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.693900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.611900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.670100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.634300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.643100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.579600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.492300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.579300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.598500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.563700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.589800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.550700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.532400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.703700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.614600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.532300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.616500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.664900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.614900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.594800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.583300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.7806919720768929, metrics={'train_runtime': 798.4124, 'train_samples_per_second': 1.002, 'train_steps_per_second': 0.125, 'total_flos': 2121514597613568.0, 'train_loss': 0.7806919720768929, 'epoch': 0.1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"gemma-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"<start_of_turn>user Given the following SQL question and context, generate a SQL query:\n",
    "\n",
    " SQL Question: Identify the number of artworks created by female artists from the 16th century and their average cultural impact score. \n",
    "\n",
    " SQL Context: CREATE TABLE Artists (ArtistID INT, ArtistName VARCHAR(50), Gender VARCHAR(10), BirthYear INT); INSERT INTO Artists (ArtistID, ArtistName, Gender, BirthYear) VALUES (1, 'Artemisia Gentileschi', 'Female', 1593); CREATE TABLE Artworks (ArtworkID INT, ArtistID INT, ArtworkName VARCHAR(50), CulturalImpactScore FLOAT); INSERT INTO Artworks (ArtworkID, ArtistID, ArtworkName, CulturalImpactScore) VALUES (1, 1, 'Judith Slaying Holofernes', 75.5);\n",
    "\n",
    " Generated SQL Query: <end_of_turn>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/niq/hcsr2001/anaconda3/envs/LLM/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/niq/hcsr2001/anaconda3/envs/LLM/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    user\n",
      "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "    user Given the following SQL question and context, generate a SQL query:\n",
      "\n",
      " SQL Question: Identify the number of artworks created by female artists from the 16th century and their average cultural impact score. \n",
      "\n",
      " SQL Context: CREATE TABLE Artists (ArtistID INT, ArtistName VARCHAR(50), Gender VARCHAR(10), BirthYear INT); INSERT INTO Artists (ArtistID, ArtistName, Gender, BirthYear) VALUES (1, 'Artemisia Gentileschi', 'Female', 1593); CREATE TABLE Artworks (ArtworkID INT, ArtistID INT, ArtworkName VARCHAR(50), CulturalImpactScore FLOAT); INSERT INTO Artworks (ArtworkID, ArtistID, ArtworkName, CulturalImpactScore) VALUES (1, 1, 'Judith Slaying Holofernes', 75.5);\n",
      "\n",
      " Generated SQL Query: \n",
      "    \n",
      "model\n",
      "    \n",
      "\n",
      "    Tại 'Artists', inser INTO Artists (ArtistID, ArtistName, Gender, BirthYear) VALUES (2, 'Raphael Da Vinci', 'Male', 1452); \n",
      "\n",
      " INSERT INTO Artworks (ArtworkID, ArtistID, ArtworkName, CulturalImpactScore) VALUES (3, 2, 'Mona Lisa', 88.2); \n",
      "\n",
      " SELECT COUNT(DISTINCT Artists.ArtistName) as num_female_artworks, AVG(Arts.CulturalImpactScore) as avg_impact FROM Artists LEFT OUTER JOIN Artworks ON Artists.ArtistID = Artworks.ArtistID WHERE Artists.Gender = 'Female' GROUP BY Artists.ArtistName HAVING COUNT(DISTINCT Artists.ArtistName) = 6; \n"
     ]
    }
   ],
   "source": [
    "result = get_completion(query=query, model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user Given the following SQL question and context, generate a SQL query:\n",
      "\n",
      " SQL Question: Identify the number of artworks created by female artists from the 16th century and their average cultural impact score. \n",
      "\n",
      " SQL Context: CREATE TABLE Artists (ArtistID INT, ArtistName VARCHAR(50), Gender VARCHAR(10), BirthYear INT); INSERT INTO Artists (ArtistID, ArtistName, Gender, BirthYear) VALUES (1, 'Artemisia Gentileschi', 'Female', 1593); CREATE TABLE Artworks (ArtworkID INT, ArtistID INT, ArtworkName VARCHAR(50), CulturalImpactScore FLOAT); INSERT INTO Artworks (ArtworkID, ArtistID, ArtworkName, CulturalImpactScore) VALUES (1, 1, 'Judith Slaying Holofernes', 75.5);\n",
      "\n",
      " Generated SQL Query: <end_of_turn>\n",
      "\n",
      "<start_of_turn> model SELECT COUNT(Artworks.ArtworkID), AVG(Artworks.CulturalImpactScore) FROM Artists INNER JOIN Artworks ON Artists.ArtistID = Artworks.ArtistID WHERE Artists.Gender = 'Female' AND Artists.BirthYear BETWEEN 1501 AND 1600; <end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][8][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the model and tokenizer to the Hugging Face Model Hub\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sri.karan\\.conda\\envs\\activeloop\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.17) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas  as pd\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai,os\n",
    "load_dotenv(r'D:\\Git\\NLP\\LLM\\ActivLoop\\.env')\n",
    "openai_api_key = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "openai.api_base = os.getenv(\"OpenAiService\")\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version =os.getenv(\"OpenAiVersion\")\n",
    "davincimodel= os.getenv(\"OpenAiDavinci\")\n",
    "active_loop_token=os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "embedding_model=os.getenv(\"OpenAiEmbedding\")\n",
    "chat_ai=os.getenv(\"ChatAI\")#\n",
    "HUGGINGFACEHUB_API_TOKEN=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "We are going to create a voice assistant for your knowledge base! This lesson will outline how you can develop your very own voice assistant employing state-of-the-art artificial intelligence tools. The voice assistant utilizes OpenAI's Whisper, a sophisticated automatic speech recognition (ASR) system. Whisper effectively transcribes our voice inputs into text. Once our voice inputs have been transcribed into text, we turn our attention towards generating voice outputs. To accomplish this, we employ Eleven Labs, which enables the voice assistant to respond to the users in an engaging and natural manner.\n",
    "\n",
    "The core of the project revolves around a robust question-answering mechanism. This process initiates with loading the vector database, a repository housing several documents relevant to our potential queries. On posing a question, the system retrieves the documents from this database and, along with the question, feeds them to the LLM. The LLM then generates the response based on retrieved documents.\n",
    "\n",
    "We aim to create a voice assistant that can efficiently navigate a knowledge base, providing precise and timely responses to a user's queries. For this experiment weâ€™re using the â€˜JarvisBaseâ€™ repository on GitHub."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/peterw/JarvisBase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tokens and APIs\n",
    "For this experiment, youâ€™d need to obtain several API keys and tokens. They need to be set in the environment variable as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY']='<your-openai-api-key>'\n",
    "os.environ['ELEVEN_API_KEY']='<your-eleven-api-key>'\n",
    "os.environ['ACTIVELOOP_TOKEN']='<your-activeloop-token>'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access OpenAI's services, you must first obtain credentials by signing up on their website, completing the registration process, and creating an API key from your dashboard. This enables you to leverage OpenAI's powerful capabilities in your projects.\n",
    "\n",
    "If you don't have an account yet, create one by going to https://platform.openai.com/. If you already have an account, skip to step 5.\n",
    "Fill out the registration form with your name, email address, and desired password.\n",
    "OpenAI will send you a confirmation email with a link. Click on the link to confirm your account.\n",
    "Please note that you'll need to verify your email account and provide a phone number for verification.\n",
    "Log in to https://platform.openai.com/.\n",
    "Navigate to the API key section at https://platform.openai.com/account/api-keys.\n",
    "Click \"Create new secret key\" and give the key a recognizable name or ID.\n",
    "\n",
    "To get the ELEVEN_API_KEY, follow these steps:\n",
    "\n",
    "1. Go to https://elevenlabs.io/ and click on \"Sign Up\" to create an account.\n",
    "2. Once you have created an account, log in and navigate to the \"API\" section.\n",
    "3. Click the \"Create API key\" button and follow the prompts to generate a new API key.\n",
    "4. Copy the API key and paste it into your code where it says \"your-eleven-api-key\" in the ELEVEN_API_KEY variable.\n",
    "\n",
    "\n",
    "For ACTIVELOOP TOKEN, follow these easy steps:\n",
    "\n",
    "Go to https://www.activeloop.ai/ and click on â€œSign Upâ€ to create an account.\n",
    "  2. Once you have an Activeloop account, you can create tokens in the Deep Lake App (Organization Details -> API Tokens)\n",
    "\n",
    " 3. Click the \"Create API key\" button and generate a new API Token.\n",
    "\n",
    "Copy the API key and paste it as your environment variable: ACTIVELOOP_TOKEN='your-Activeloop-token'\n",
    "1. Sourcing Content from Hugging Face Hub\n",
    "Now that everything is set up, letâ€™s begin by aggregating all Python library articles from the Hugging Face Hub, an open platform to share, collaborate and advance in machine learning. These articles will serve as the knowledge base for our voice assistant. We'll do some web scraping in order to collect some knowledge documents.\n",
    "\n",
    "Letâ€™s observe and run the script.py file (i.e. run python scrape.py ). This script contains all the code we use in this lesson under the â€œSourcing Content from Hugging Face Hubâ€ and â€œEmbedding and storing in Deep Lakeâ€ sections. You can fork or download the mentioned repository and run the files.\n",
    "\n",
    "We start with importing necessary modules, loading environment variables, and setting up the path for Deep Lake, a vector database. It also sets up an OpenAIEmbeddings instance, which will be used later to embed the scraped articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import re\n",
    "\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"hayagriva99999\"\n",
    "my_activeloop_dataset_name = \"jarvisAssistant\"\n",
    "dataset_path= f'hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}'\n",
    "\n",
    "embeddings =  OpenAIEmbeddings(deployment=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TextEmbeddingAda002'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a list of relative URLs leading to knowledge documents hosted on the Hugging Face Hub. To do this, we define a function called get_documentation_urls(). Using another function, construct_full_url(), we then append these relative URLs to the base URL of the Hugging Face Hub, effectively creating full URLs that we can access directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documentation_urls():\n",
    "    # List of relative URLs for Hugging Face documentation pages, commented a lot of these because it would take too long to scrape all of them\n",
    "    return [\n",
    "\t\t    '/docs/huggingface_hub/guides/overview',\n",
    "\t\t    '/docs/huggingface_hub/guides/download',\n",
    "\t\t    # '/docs/huggingface_hub/guides/upload',\n",
    "\t\t    # '/docs/huggingface_hub/guides/hf_file_system',\n",
    "\t\t    # '/docs/huggingface_hub/guides/repository',\n",
    "\t\t    # '/docs/huggingface_hub/guides/search',\n",
    "\t\t    # You may add additional URLs here or replace all of them\n",
    "    ]\n",
    "\n",
    "def construct_full_url(base_url, relative_url):\n",
    "    # Construct the full URL by appending the relative URL to the base URL\n",
    "    return base_url + relative_url"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script then aggregates all the scraped content from the URLs. This is achieved with the scrape_all_content() function, which iteratively calls scrape_page_content() for each URL and extracts its text. This collected text is then saved to a file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_content(url):\n",
    "    # Send a GET request to the URL and parse the HTML response using BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract the desired content from the page (in this case, the body text)\n",
    "    text=soup.body.text.strip()\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', text)\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def scrape_all_content(base_url, relative_urls, filename,root_dir):\n",
    "    # Loop through the list of URLs, scrape content and add it to the content list\n",
    "    content = []\n",
    "    for relative_url in relative_urls:\n",
    "        full_url = construct_full_url(base_url, relative_url)\n",
    "        scraped_content = scrape_page_content(full_url)\n",
    "        content.append(scraped_content.rstrip('\\n'))\n",
    "\n",
    "    # Write the scraped content to a file\n",
    "    with open(os.path.join(root_dir,filename), 'w', encoding='utf-8') as file:\n",
    "        for item in content:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and splitting texts\n",
    "To prepare the collected text for embedding into our vector database, we load the content from the file and split it into separate documents using the load_docs() function. To further refine the content, we split it into individual chunks through the split_docs(). Here weâ€™d see a Text loader and text_splitter in action. \n",
    "\n",
    "The instructiontext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) creates an instance of a text splitter that splits the text into chunks based on characters. Each document in docs is split into chunks of approximately 1000 characters, with no overlap between consecutive chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load documents from a file\n",
    "def load_docs(root_dir,filename):\n",
    "    # Create an empty list to hold the documents\n",
    "    docs = []\n",
    "    try:\n",
    "        # Load the file using the TextLoader class and UTF-8 encoding\n",
    "        loader = TextLoader(os.path.join(\n",
    "            root_dir, filename), encoding='utf-8')\n",
    "        # Split the loaded file into separate documents and add them to the list of documents\n",
    "        docs.extend(loader.load_and_split())\n",
    "    except Exception as e:\n",
    "        # If an error occurs during loading, ignore it and return an empty list of documents\n",
    "        pass\n",
    "    # Return the list of documents\n",
    "    return docs\n",
    "  \n",
    "def split_docs(docs):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Embedding and storing in Deep Lake\n",
    "Once we've collected the necessary articles, the next step is to embed them using Deep Lake. Deep Lake is a powerful tool for creating searchable vector databases. In this context, it will allow us to efficiently index and retrieve the information contained in our Python library articles.\n",
    "\n",
    "Finally, we're ready to populate our vector database.\n",
    "\n",
    "The Deep Lake integration initializes a database instance with the given dataset path and the predefined OpenAIEmbeddings function. The OpenAIEmbeddings is converting the text chunks into their embedding vectors, a format suitable for the vector database. The .add_documents method will process and store the texts on the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://hayagriva99999/jarvisAssistant\n",
      "Deep Lake Dataset in hub://hayagriva99999/jarvisAssistant already exists, loading from the storage\n",
      "[Document(page_content=\"Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation How-to guides Hub Python Library Search documentation mainv0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 EN Get started Home Quickstart Installation How-to guides Overview Download files Upload files HfFileSystem Repository Search Inference Community Tab Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started How-to guides In this section, you will find practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface_hub to solve real-world problems: Repository How to create a repository on the Hub? How to configure it? How to interact with it? Download files How do I download a file from the Hub? How do I download a repository? Upload files How to upload a file or a folder? How to make changes to an existing repository on the Hub? Search How to efficiently search through the 200k+ public models, datasets and spaces? HfFileSystem How to interact with the Hub through a convenient interface that mimics Python's file interface? Inference How to make predictions using the accelerated Inference API? Community Tab How to interact with the Community tab (Discussions and Pull Requests)? Cache How does the cache-system work? How to benefit from it? Model Cards How to create and share Model Cards? Manage your Space How to manage your Space hardware and configuration? Integrate a library What does it mean to integrate a library with the Hub? And how to do it? Webhooks server How to create a server to receive Webhooks and deploy it as a Space? â†Installation Download filesâ†’ How-to guides\", metadata={'source': 'D://Git//NLP//LLM//ActivLoop//data//voice_data//content.txt'}), Document(page_content='Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Download files from the Hub Hub Python Library Search documentation mainv0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 EN Get started Home Quickstart Installation How-to guides Overview Download files Upload files HfFileSystem Repository Search Inference Community Tab Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Download files from the Hub The huggingface_hub library provides functions to download files from the repositories stored on the Hub. You can use these functions independently or integrate them into your own library, making it more convenient for your users to interact with the Hub. This guide will show you how to: Download and cache a single file. Download and cache an entire repository. Download files to a local folder. Download a single file The hf_hub_download() function is the main function for downloading files from the Hub. It downloads the remote file, caches it on disk (in a version-aware way), and returns its local file path. The returned filepath is a pointer to the HF local cache. Therefore, it is important to not modify the file to avoid having a corrupted cache. If you are interested in getting to know more about how files are cached, please refer to our caching guide. From latest version Select the file to download using the repo_id, repo_type and filename parameters. By default, the file will be considered as being part of a model repo. Copied >>> from huggingface_hub import hf_hub_download >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\") \\'/root/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade/config.json\\' # Download from a dataset >>> hf_hub_download(repo_id=\"google/fleurs\", filename=\"fleurs.py\", repo_type=\"dataset\") \\'/root/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34/fleurs.py\\' From specific version By default, the latest version from the main branch is downloaded. However, in some cases you want to download a file at a particular version (e.g. from a specific branch, a PR, a tag or a commit hash). To do so, use the revision parameter: Copied # Download from the `v1.0` tag >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"v1.0\") # Download from the `test-branch` branch >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"test-branch\") # Download from Pull Request #3 >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"refs/pr/3\") # Download from a specific commit hash >>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"877b84a8f93f2d619faa2a6e514a32beef88ab0a\") Note: When using the commit hash, it must be the full-length hash instead of a 7-character commit hash. Construct a download URL In case you want to construct the URL used to download a file from a repo, you can use hf_hub_url() which returns a URL. Note that it is used internally by hf_hub_download(). Download an entire repository snapshot_download() downloads an entire repository at a given revision. It uses internally hf_hub_download() which means all downloaded files are also cached on your local disk. Downloads', metadata={'source': 'D://Git//NLP//LLM//ActivLoop//data//voice_data//content.txt'}), Document(page_content='repository snapshot_download() downloads an entire repository at a given revision. It uses internally hf_hub_download() which means all downloaded files are also cached on your local disk. Downloads are made concurrently to speed-up the process. To download a whole repository, just pass the repo_id and repo_type: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\") \\'/home/lysandre/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade\\' # Or from a dataset >>> snapshot_download(repo_id=\"google/fleurs\", repo_type=\"dataset\") \\'/home/lysandre/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34\\' snapshot_download() downloads the latest revision by default. If you want a specific repository revision, use the revision parameter: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", revision=\"refs/pr/1\") Filter files to download snapshot_download() provides an easy way to download a repository. However, you donâ€™t always want to download the entire content of a repository. For example, you might want to prevent downloading all .bin files if you know youâ€™ll only use the .safetensors weights. You can do that using allow_patterns and ignore_patterns parameters. These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing patterns) as documented here. The pattern matching is based on fnmatch. For example, you can use allow_patterns to only download JSON configuration files: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", allow_patterns=\"*.json\") On the other hand, ignore_patterns can exclude certain files from being downloaded. The following example ignores the .msgpack and .h5 file extensions: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", ignore_patterns=[\"*.msgpack\", \"*.h5\"]) Finally, you can combine both to precisely filter your download. Here is an example to download all json and markdown files except vocab.json. Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id=\"gpt2\", allow_patterns=[\"*.md\", \"*.json\"], ignore_patterns=\"vocab.json\") Download file(s) to local folder The recommended (and default) way to download files from the Hub is to use the cache-system. You can define your cache location by setting cache_dir parameter (both in hf_hub_download() and snapshot_download()). However, in some cases you want to download files and move them to a specific folder. This is useful to get a workflow closer to what git commands offer. You can do that using the local_dir and local_dir_use_symlinks parameters: local_dir must be a path to a folder on your system. The downloaded files will keep the same file structure as in the repo. For example if filename=\"data/train.csv\" and local_dir=\"path/to/folder\", then the returned filepath will be \"path/to/folder/data/train.csv\". local_dir_use_symlinks defines how the file must be saved in your local folder.The default behavior (\"auto\") is to duplicate small files (<5MB) and use symlinks for bigger files. Symlinks allow to optimize both bandwidth and disk usage. However manually editing a symlinked file might corrupt the cache, hence the duplication for small files. The 5MB threshold can be configured with the HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD environment variable. If local_dir_use_symlinks=True is set, all files are symlinked for an optimal disk space optimization. This is for example useful when downloading a huge dataset with thousands of small files. Finally, if you donâ€™t want symlinks at all you can disable them (local_dir_use_symlinks=False). The cache directory will still be used to check wether the file is already cached or not. If already cached, the file is duplicated from the', metadata={'source': 'D://Git//NLP//LLM//ActivLoop//data//voice_data//content.txt'}), Document(page_content='all you can disable them (local_dir_use_symlinks=False). The cache directory will still be used to check wether the file is already cached or not. If already cached, the file is duplicated from the cache (i.e. saves bandwidth but increases disk usage). If the file is not already cached, it will be downloaded and moved directly to the local dir. This means that if you need to reuse it somewhere else later, it will be re-downloaded. Here is a table that summarizes the different options to help you choose the parameters that best suit your use case. Parameters File already cached Returned path Can read path? Can save to path? Optimized bandwidth Optimized disk usage local_dir=None symlink in cache âœ… âŒ(save would corrupt the cache) âœ… âœ… local_dir=\"path/to/folder\"local_dir_use_symlinks=\"auto\" file or symlink in folder âœ… âœ… (for small files) âš ï¸ (for big files do not resolve path before saving) âœ… âœ… local_dir=\"path/to/folder\"local_dir_use_symlinks=True symlink in folder âœ… âš ï¸(do not resolve path before saving) âœ… âœ… local_dir=\"path/to/folder\"local_dir_use_symlinks=False No file in folder âœ… âœ… âŒ(if re-run, file is re-downloaded) âš ï¸(multiple copies if ran in multiple folders) local_dir=\"path/to/folder\"local_dir_use_symlinks=False Yes file in folder âœ… âœ… âš ï¸(file has to be cached first) âŒ(file is duplicated) Note: if you are on a Windows machine, you need to enable developer mode or run huggingface_hub as admin to enable symlinks. Check out the cache limitations section for more details. â†Overview Upload filesâ†’ Download files from the Hub Download a single file From latest version From specific version Construct a download URL Download an entire repository Filter files to download Download file(s) to local folder', metadata={'source': 'D://Git//NLP//LLM//ActivLoop//data//voice_data//content.txt'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://hayagriva99999/jarvisAssistant', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "# Define the main function\n",
    "def main():\n",
    "    base_url = 'https://huggingface.co'\n",
    "    # Set the name of the file to which the scraped content will be saved\n",
    "    filename='content.txt'\n",
    "    # Set the root directory where the content file will be saved\n",
    "    root_dir ='D://Git//NLP//LLM//ActivLoop//data//voice_data//'\n",
    "    relative_urls = get_documentation_urls()\n",
    "    # Scrape all the content from the relative URLs and save it to the content file\n",
    "    content = scrape_all_content(base_url, relative_urls,filename,root_dir)\n",
    "    # Load the content from the file\n",
    "    docs = load_docs(root_dir,filename)\n",
    "    # Split the content into individual documents\n",
    "    texts = split_docs(docs)\n",
    "    # Create a DeepLake database with the given dataset path and embedding function\n",
    "    print(dataset_path)\n",
    "    db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "    # Add the individual documents to the database\n",
    "    print(texts)\n",
    "    db.add_documents(texts)\n",
    "    # Clean up by deleting the content file\n",
    "    os.remove(filename)\n",
    "\n",
    "# Call the main function if this script is being run as the main program\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Voice Assistant\n",
    "Having successfully stored all the necessary data in the vector database, in this instance using Deep Lake by Activeloop, we're ready to utilize this data in our chatbot.\n",
    "\n",
    "Without further ado, let's transition to the coding part of our chatbot. The following code can be found in the chat.py file of the directory. To give it a try, run streamlit run chat.py.\n",
    "\n",
    "These libraries will help us in building web applications with Streamlit, handling audio input, generating text responses, and effectively retrieving information stored in the Deep Lake:\n",
    "\n",
    "imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import streamlit as st\n",
    "from audio_recorder_streamlit import audio_recorder\n",
    "from elevenlabs import generate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from streamlit_chat import message\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "TEMP_AUDIO_PATH = \"temp_audio.wav\"\n",
    "AUDIO_FORMAT = \"audio/wav\"\n",
    "\n",
    "# Load environment variables from .env file and return the keys\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "eleven_api_key = os.environ.get('ELEVEN_API_KEY')\n",
    "active_loop_data_set_path = os.environ.get('DEEPLAKE_DATASET_PATH')\n",
    "\n",
    "# Load embeddings and DeepLake database\n",
    "def load_embeddings_and_database(active_loop_data_set_path):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = DeepLake(\n",
    "        dataset_path=active_loop_data_set_path,\n",
    "        read_only=True,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return db\n",
    "\n",
    "# Transcribe audio using OpenAI Whisper API\n",
    "def transcribe_audio(audio_file_path, openai_key):\n",
    "    openai.api_key = openai_key\n",
    "    try:\n",
    "        with open(audio_file_path, \"rb\") as audio_file:\n",
    "            response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "        return response[\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Whisper API: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Record audio using audio_recorder and transcribe using transcribe_audio\n",
    "def record_and_transcribe_audio():\n",
    "    audio_bytes = audio_recorder()\n",
    "    transcription = None\n",
    "    if audio_bytes:\n",
    "        st.audio(audio_bytes, format=AUDIO_FORMAT)\n",
    "\n",
    "        with open(TEMP_AUDIO_PATH, \"wb\") as f:\n",
    "            f.write(audio_bytes)\n",
    "\n",
    "        if st.button(\"Transcribe\"):\n",
    "            transcription = transcribe_audio(TEMP_AUDIO_PATH, openai.api_key)\n",
    "            os.remove(TEMP_AUDIO_PATH)\n",
    "            display_transcription(transcription)\n",
    "\n",
    "    return transcription\n",
    "\n",
    "# Display the transcription of the audio on the app\n",
    "def display_transcription(transcription):\n",
    "    if transcription:\n",
    "        st.write(f\"Transcription: {transcription}\")\n",
    "        with open(\"audio_transcription.txt\", \"w+\") as f:\n",
    "            f.write(transcription)\n",
    "    else:\n",
    "        st.write(\"Error transcribing audio.\")\n",
    "\n",
    "# Get user input from Streamlit text input field\n",
    "def get_user_input(transcription):\n",
    "    return st.text_input(\"\", value=transcription if transcription else \"\", key=\"input\")\n",
    "\n",
    "# Search the database for a response based on the user's query\n",
    "def search_db(user_input, db):\n",
    "    print(user_input)\n",
    "    retriever = db.as_retriever()\n",
    "    retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "    retriever.search_kwargs['fetch_k'] = 100\n",
    "    retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "    retriever.search_kwargs['k'] = 10\n",
    "    model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "    qa = RetrievalQA.from_llm(model, retriever=retriever, return_source_documents=True)\n",
    "    return qa({'query': user_input})\n",
    "\n",
    "# Display conversation history using Streamlit messages\n",
    "def display_conversation(history):\n",
    "    for i in range(len(history[\"generated\"])):\n",
    "        message(history[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
    "        message(history[\"generated\"][i],key=str(i))\n",
    "        #Voice using Eleven API\n",
    "        voice= \"Bella\"\n",
    "        text= history[\"generated\"][i]\n",
    "        audio = generate(text=text, voice=voice,api_key=eleven_api_key)\n",
    "        st.audio(audio, format='audio/mp3')\n",
    "\n",
    "# Main function to run the app\n",
    "def main():\n",
    "    # Initialize Streamlit app with a title\n",
    "    st.write(\"# JarvisBase ðŸ§™\")\n",
    "   \n",
    "    # Load embeddings and the DeepLake database\n",
    "    db = load_embeddings_and_database(active_loop_data_set_path)\n",
    "\n",
    "    # Record and transcribe audio\n",
    "    transcription = record_and_transcribe_audio()\n",
    "\n",
    "    # Get user input from text input or audio transcription\n",
    "    user_input = get_user_input(transcription)\n",
    "\n",
    "    # Initialize session state for generated responses and past messages\n",
    "    if \"generated\" not in st.session_state:\n",
    "        st.session_state[\"generated\"] = [\"I am ready to help you\"]\n",
    "    if \"past\" not in st.session_state:\n",
    "        st.session_state[\"past\"] = [\"Hey there!\"]\n",
    "        \n",
    "    # Search the database for a response based on user input and update session state\n",
    "    if user_input:\n",
    "        output = search_db(user_input, db)\n",
    "        print(output['source_documents'])\n",
    "        st.session_state.past.append(user_input)\n",
    "        response = str(output[\"result\"])\n",
    "        st.session_state.generated.append(response)\n",
    "\n",
    "    # Display conversation history using Streamlit messages\n",
    "    if st.session_state[\"generated\"]:\n",
    "        display_conversation(st.session_state)\n",
    "\n",
    "# Run the main function when the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import streamlit as st\n",
    "from audio_recorder_streamlit import audio_recorder\n",
    "from elevenlabs import generate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from streamlit_chat import message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TEMP_AUDIO_PATH = r\"D:\\Git\\NLP\\LLM\\ActivLoop\\data\\voice_data\\temp_audio.wav\"\n",
    "AUDIO_FORMAT = \"audio/wav\"\n",
    "\n",
    "# Load environment variables from .env file and return the keys\n",
    "openai.api_key = os.environ.get('OPENAI_API_KEY')\n",
    "eleven_api_key = os.environ.get('ELEVEN_API_KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create an instance that points to our Deep Lake vector database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_loop_data_set_path=\"hub://hayagriva99999/jarvisAssistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_database(active_loop_data_set_path):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = DeepLake(\n",
    "        dataset_path=\"hub://hayagriva99999/jarvisAssistant\",\n",
    "        read_only=True,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file_path, openai_key):\n",
    "    openai.api_key = openai_key\n",
    "    try:\n",
    "        with open(audio_file_path, \"rb\") as audio_file:\n",
    "            response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "        return response[\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Whisper API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transcribes an audio file into text using the OpenAI Whisper API, requiring the path of the audio file and the OpenAI key as input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record audio using audio_recorder and transcribe using transcribe_audio\n",
    "def record_and_transcribe_audio():\n",
    "    audio_bytes = audio_recorder()\n",
    "    transcription = None\n",
    "    if audio_bytes:\n",
    "        st.audio(audio_bytes, format=AUDIO_FORMAT)\n",
    "\n",
    "        with open(TEMP_AUDIO_PATH, \"wb\") as f:\n",
    "            f.write(audio_bytes)\n",
    "\n",
    "        if st.button(\"Transcribe\"):\n",
    "            transcription = transcribe_audio(TEMP_AUDIO_PATH, openai.api_key)\n",
    "            os.remove(TEMP_AUDIO_PATH)\n",
    "            display_transcription(transcription)\n",
    "\n",
    "    return transcription\n",
    "\n",
    "# Display the transcription of the audio on the app\n",
    "def display_transcription(transcription):\n",
    "    if transcription:\n",
    "        st.write(f\"Transcription: {transcription}\")\n",
    "        with open(\"audio_transcription.txt\", \"w+\") as f:\n",
    "            f.write(transcription)\n",
    "    else:\n",
    "        st.write(\"Error transcribing audio.\")\n",
    "\n",
    "# Get user input from Streamlit text input field\n",
    "def get_user_input(transcription):\n",
    "    return st.text_input(\"\", value=transcription if transcription else \"\", key=\"input\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code allows users to record audio directly within the application. The recorded audio is then transcribed into text using the Whisper API, and the transcribed text is displayed on the application. If any issues occur during the transcription process, an error message will be shown to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the database for a response based on the user's query\n",
    "def search_db(user_input, db):\n",
    "    print(user_input)\n",
    "    retriever = db.as_retriever()\n",
    "    retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "    retriever.search_kwargs['fetch_k'] = 100\n",
    "    retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "    retriever.search_kwargs['k'] = 4\n",
    "    model = ChatOpenAI(engine=chat_ai)\n",
    "    qa = RetrievalQA.from_llm(model, retriever=retriever, return_source_documents=True)\n",
    "    return qa({'query': user_input})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment of the code is for searching the vector database for the most relevant responses to the user's query. It first converts the database into a retriever, which is a tool that searches for the nearest embeddings in the vector space. It then sets various parameters for the search, such as the metric to use when measuring distance in the embedding space, the number of documents to fetch initially, whether or not to use maximal marginal relevance to balance the diversity and relevance of the results, and how many results to return. The retrieved results are then passed through the language model, which is GPT-3.5 Turbo in this case, to generate the most appropriate response to the user's query.\n",
    "\n",
    "### Streamlit\n",
    "Streamlit is a Python framework used for building data visualization web applications. It provides an intuitive way to create interactive web apps for machine learning and data science projects.\n",
    "\n",
    "Now we have the part with the conversation history between the user and the chatbot using Streamlit's messaging functionality. It goes through the previous messages in the conversation and displays each user message followed by the corresponding chatbot response. It employs the Eleven Labs API to convert the chatbot's text response into speech and give the chatbot a voice. This voice output, in MP3 format, is then played on the Streamlit interface, adding an auditory dimension to the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display conversation history using Streamlit messages\n",
    "def display_conversation(history):\n",
    "    for i in range(len(history[\"generated\"])):\n",
    "        message(history[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
    "        message(history[\"generated\"][i],key=str(i))\n",
    "        #Voice using Eleven API\n",
    "        voice= \"Bella\"\n",
    "        text= history[\"generated\"][i]\n",
    "        audio = generate(text=text, voice=voice,api_key=eleven_api_key)\n",
    "        st.audio(audio, format='audio/mp3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interaction\n",
    "After the knowledge base is set up, the next stage is user interaction. The voice assistant is designed to accept queries either in the form of voice recordings or typed text.\n",
    "\n",
    "# Main function to run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize Streamlit app with a title\n",
    "    st.write(\"# JarvisBase ðŸ§™\")\n",
    "   \n",
    "    # Load embeddings and the DeepLake database\n",
    "    db = load_embeddings_and_database(dataset_path)\n",
    "\n",
    "    # Record and transcribe audio\n",
    "    transcription = record_and_transcribe_audio()\n",
    "\n",
    "    # Get user input from text input or audio transcription\n",
    "    user_input = get_user_input(transcription)\n",
    "\n",
    "    # Initialize session state for generated responses and past messages\n",
    "    if \"generated\" not in st.session_state:\n",
    "        st.session_state[\"generated\"] = [\"I am ready to help you\"]\n",
    "    if \"past\" not in st.session_state:\n",
    "        st.session_state[\"past\"] = [\"Hey there!\"]\n",
    "        \n",
    "    # Search the database for a response based on user input and update the session state\n",
    "    if user_input:\n",
    "        output = search_db(user_input, db)\n",
    "        print(output['source_documents'])\n",
    "        st.session_state.past.append(user_input)\n",
    "        response = str(output[\"result\"])\n",
    "        st.session_state.generated.append(response)\n",
    "\n",
    "    #Display conversation history using Streamlit messages\n",
    "    if st.session_state[\"generated\"]:\n",
    "        display_conversation(st.session_state)\n",
    "\n",
    "# Run the main function when the script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main driver of the entire application. First, it sets up the Streamlit application and loads the Deep Lake vector database along with its embeddings. It then offers two methods for user input: through text or through an audio recording which is then transcribed.\n",
    "\n",
    "The application keeps a record of past user inputs and generated responses in a session state. When new user input is received, the application searches the database for the most suitable response. This response is then added to the session state.\n",
    "\n",
    "Finally, the application displays the entire conversation history, including both user inputs and chatbot responses. If the input was made via voice, the chatbot's responses are also generated in an audio format using the Eleven Labs API.\n",
    "\n",
    "You should now run the following command in your terminal:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run your application using the Streamlit command, the application will start a local web server and provide you with a URL where your application is running and can be accessed via a web browser. In your case, you have two URLs: a Network URL and an External URL.\n",
    "\n",
    "Your application will be running as long as the command is running in your terminal, and it will stop once you stop the command (ctrl+Z) or close the terminal.\n",
    "\n",
    "### Trying Out the UI\n",
    "We have now explained the main code parts and are ready to test the Streamlit app!\n",
    "\n",
    "This is how it presents itself.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By clicking on the microphone icon, your microphone will be active for some seconds and youâ€™ll be able to ask a question. Letâ€™s try â€œHow do I search for models in the Hugging Face Hub?â€.\n",
    "\n",
    "After a few seconds, the app will show an audio player that can be used to listen to your registered audio. You may then click on the â€œTranscribeâ€ button."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This button will invoke a call to the Whisper API and transcribe your audio. The produced text will be soon pasted to the chat text entry underneath."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the Whisper API didnâ€™t do a perfect job at transcribing â€œHugging Faceâ€ correctly and instead wrote â€œHuggy Faceâ€. This is unwanted, but letâ€™s see if ChatGPT is still able to understand the query and give it an appropriate answer by leveraging the knowledge documents stored in Deep Lake.\n",
    "\n",
    "After a few more seconds, the underlying chat will be populated with your audio transcription, along with the chatbot's textual response and its audio version, generated by calling the ElevenLabs API. As we can see, ChatGPT was smart enough to understand that â€œHuggy Faceâ€ was a misspelling of â€œHugging Faceâ€ and was still able to give an appropriate answer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this lesson we integrated several popular generative AI tools and models, namely OpenAI Whisper and ElevenLabs text-to-speech.\n",
    "\n",
    "In the next lesson weâ€™ll see how LLMs can be used to aid in understanding new codebases, such as the Twitter Algorithm public repository.\n",
    "\n",
    "Github Repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activeloop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
